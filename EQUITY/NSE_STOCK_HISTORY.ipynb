{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ckt0jUtWgbRJ",
        "jjUyS0iggf9c",
        "d73CkGvxgj_T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies "
      ],
      "metadata": {
        "id": "ckt0jUtWgbRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import requests\n",
        "import datetime\n",
        "import json\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import concurrent\n",
        "from concurrent.futures import ALL_COMPLETED"
      ],
      "metadata": {
        "id": "dD3idp0kMB-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Environment Variables "
      ],
      "metadata": {
        "id": "jjUyS0iggf9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HISTORICAL_DATA_URL = 'https://www.nseindia.com/api/historical/cm/equity?series=[%22EQ%22]&'\n",
        "BASE_URL = 'https://www.nseindia.com/'"
      ],
      "metadata": {
        "id": "Mdqz-jhKJZUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Helper Functions "
      ],
      "metadata": {
        "id": "d73CkGvxgj_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adjusted_headers():\n",
        "    return {\n",
        "        'Host': 'www.nseindia.com',\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:85.0) Gecko/20100101 Firefox/85.0',\n",
        "        'Accept': '*/*',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'X-Requested-With': 'XMLHttpRequest',\n",
        "        'DNT': '1',\n",
        "        'Connection': 'keep-alive',\n",
        "    }\n",
        "\n",
        "def fetch_cookies():\n",
        "    response = requests.get(BASE_URL, timeout=30, headers=get_adjusted_headers())\n",
        "    if response.status_code != requests.codes.ok:\n",
        "        # logging.error(\"Fetched url: %s with status code: %s and response from server: %s\" % (\n",
        "        #     BASE_URL, response.status_code, response.content))\n",
        "        raise ValueError(\"Please try again in a minute.\")\n",
        "    return response.cookies.get_dict()\n",
        "\n",
        "def fetch_url(url, cookies):\n",
        "    \"\"\"\n",
        "        This is the function call made by each thread. A get request is made for given start and end date, response is\n",
        "        parsed and dataframe is returned\n",
        "    \"\"\"\n",
        "    response = requests.get(url, timeout=30, headers=get_adjusted_headers(), cookies=cookies)\n",
        "    if response.status_code == requests.codes.ok:\n",
        "        json_response = json.loads(response.content)\n",
        "        return pd.DataFrame.from_dict(json_response['data'])\n",
        "    else:\n",
        "        raise ValueError(\"Please try again in a minute.\")\n",
        "\n",
        "def scrape_data(start_date, end_date, name=None, input_type='stock'):\n",
        "    \"\"\"\n",
        "    Called by stocks and indices to scrape data.\n",
        "    Create threads for different requests, parses data, combines them and returns dataframe\n",
        "    Args:\n",
        "        start_date (datetime.datetime): start date\n",
        "        end_date (datetime.datetime): end date\n",
        "        input_type (str): Either 'stock' or 'index'\n",
        "        name (str, optional): stock symbol or index name. Defaults to None.\n",
        "    Returns:\n",
        "        Pandas DataFrame: df containing data for stocksymbol for provided date range\n",
        "    \"\"\"\n",
        "    cookies = fetch_cookies()\n",
        "\n",
        "    start_date = datetime.datetime.strptime(start_date, \"%d-%m-%Y\")\n",
        "    end_date = datetime.datetime.strptime(end_date, \"%d-%m-%Y\")\n",
        "\n",
        "    threads, url_list = [], []\n",
        "\n",
        "    # set the window size to one year\n",
        "    window_size = datetime.timedelta(days=50)\n",
        "\n",
        "    current_window_start = start_date\n",
        "    while current_window_start < end_date:\n",
        "        current_window_end = current_window_start + window_size\n",
        "        \n",
        "        # check if the current window extends beyond the end_date\n",
        "        if current_window_end > end_date:\n",
        "            current_window_end = end_date\n",
        "        \n",
        "        st = current_window_start.strftime('%d-%m-%Y')\n",
        "        et = current_window_end.strftime('%d-%m-%Y')\n",
        "        # print(st,et)\n",
        "        if input_type == 'stock':\n",
        "            params = {'symbol': name,\n",
        "                        'from': st,\n",
        "                        'to': et}\n",
        "            url = HISTORICAL_DATA_URL + urllib.parse.urlencode(params)\n",
        "            url_list.append(url)\n",
        "\n",
        "        # move the window start to the next day after the current window end\n",
        "        current_window_start = current_window_end + datetime.timedelta(days=1)\n",
        "\n",
        "    # print(\"url_list\",url_list)\n",
        "\n",
        "    result = pd.DataFrame()\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        future_to_url = {executor.submit(fetch_url, url, cookies): url for url in url_list}\n",
        "        concurrent.futures.wait(future_to_url, return_when=ALL_COMPLETED)\n",
        "        for future in concurrent.futures.as_completed(future_to_url):\n",
        "            url = future_to_url[future]\n",
        "            try:\n",
        "                df = future.result()\n",
        "                result = pd.concat([result, df])\n",
        "            except Exception as exc:\n",
        "                # logging.error('%r generated an exception: %s. Please try again later.' % (url, exc))\n",
        "                raise exc\n",
        "    return format_dataframe_result(result, start_date, end_date)\n",
        "\n",
        "\n",
        "def format_dataframe_result(result, start_date, end_date):\n",
        "    if result.empty:\n",
        "        return f\"No Data Found : for date range {start_date} to {end_date}\"\n",
        "    columns_required = [\"CH_TIMESTAMP\", \"CH_SYMBOL\", \"CH_SERIES\", \"CH_TRADE_HIGH_PRICE\",\n",
        "                        \"CH_TRADE_LOW_PRICE\", \"CH_OPENING_PRICE\", \"CH_CLOSING_PRICE\", \"CH_LAST_TRADED_PRICE\",\n",
        "                        \"CH_PREVIOUS_CLS_PRICE\", \"CH_TOT_TRADED_QTY\", \"CH_TOT_TRADED_VAL\", \"CH_52WEEK_HIGH_PRICE\",\n",
        "                        \"CH_52WEEK_LOW_PRICE\"]\n",
        "    result = result[columns_required]\n",
        "    result = result.set_axis(\n",
        "        ['Date', 'Symbol', 'Series', 'High Price', 'Low Price', 'Open Price', 'Close Price', 'Last Price',\n",
        "         'Prev Close Price', 'Total Traded Quantity', 'Total Traded Value', '52 Week High Price',\n",
        "         '52 Week Low Price'], axis=1)\n",
        "    result['Date'] = pd.to_datetime(result['Date'])\n",
        "    result = result.sort_values('Date', ascending=True)\n",
        "    result.reset_index(drop=True, inplace=True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "qsKLL6rD-JfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Directly to DataFrame "
      ],
      "metadata": {
        "id": "bNp-cN2rgq47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_data('15-01-2000','16-01-2000','MRF')"
      ],
      "metadata": {
        "id": "zYhPQToRDVzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}